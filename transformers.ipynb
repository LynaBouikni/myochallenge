{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from src.envs.environment_factory import EnvironmentFactory\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "#### OBTAIN TRAJECTORIES\n",
    "\n",
    "env_name = \"CustomMyoBaodingBallsP1\"\n",
    "\n",
    "# Path to normalized Vectorized environment (if not first task)\n",
    "PATH_TO_NORMALIZED_ENV = \"output/training/2022-10-02/21-36-47/training_env.pkl\"  # \"trained_models/normalized_env_original\"\n",
    "\n",
    "# Path to pretrained network (if not first task)\n",
    "PATH_TO_PRETRAINED_NET = \"output/training/2022-10-02/21-36-47/best_model.zip\"  # \"trained_models/best_model.zip\"\n",
    "\n",
    "# Reward structure and task parameters:\n",
    "config = {\n",
    "    \"weighted_reward_keys\": {\n",
    "        \"pos_dist_1\": 0,\n",
    "        \"pos_dist_2\": 0,\n",
    "        \"act_reg\": 0,\n",
    "        \"alive\": 0,\n",
    "        \"solved\": 5,\n",
    "        \"done\": 0,\n",
    "        \"sparse\": 0,\n",
    "    },\n",
    "    \"goal_time_period\": [4, 6],\n",
    "    \"task\": \"ccw\",\n",
    "    \"enable_rhi\": False,\n",
    "    \"enable_rsi\": False\n",
    "}\n",
    "\n",
    "\n",
    "# Function that creates and monitors vectorized environments:\n",
    "def make_parallel_envs(env_name, env_config, num_env, start_index=0):\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = EnvironmentFactory.create(env_name, **env_config)\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])\n",
    "\n",
    "\n",
    "\n",
    "###### TRANSFORMER\n",
    "\n",
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 86  # size of state space\n",
    "    act_dim: int = 39  # size of action space\n",
    "    max_ep_len: int = 200 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }\n",
    "\n",
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create vectorized environments:\n",
    "    envs = make_parallel_envs(env_name, config, num_env=16)\n",
    "\n",
    "    # Normalize environment:\n",
    "    envs = VecNormalize.load(PATH_TO_NORMALIZED_ENV, envs)\n",
    "\n",
    "    # Create model (hyperparameters from RL Zoo HalfCheetak)\n",
    "    model = RecurrentPPO.load(PATH_TO_PRETRAINED_NET, env=envs)\n",
    "\n",
    "    # EVALUATE\n",
    "    eval_model = model\n",
    "    eval_env = EnvironmentFactory.create(env_name, **config)\n",
    "\n",
    "    # Enjoy trained agent\n",
    "    num_episodes = 5000\n",
    "    perfs = []\n",
    "    all_observations, all_actions, all_rewards, all_dones = [] , [] , [] , []\n",
    "    for i in range(num_episodes):\n",
    "        observations, actions, rewards, dones = [] , [] , [] , []\n",
    "        lstm_states = None\n",
    "        cum_rew = 0\n",
    "        obs = eval_env.reset()\n",
    "        episode_starts = np.ones((1,), dtype=bool)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, lstm_states = eval_model.predict(\n",
    "                envs.normalize_obs(obs),\n",
    "                state=lstm_states,\n",
    "                episode_start=episode_starts,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            obs, rew, done, _ = eval_env.step(action)\n",
    "\n",
    "            observations.append(obs.tolist())\n",
    "            actions.append(action.tolist())\n",
    "            rewards.append(rew.tolist())\n",
    "            dones.append(done)\n",
    "            episode_starts = done\n",
    "            cum_rew += rew\n",
    "        perfs.append(cum_rew)\n",
    "        all_observations.append(observations)\n",
    "        all_actions.append(actions)\n",
    "        all_rewards.append(rewards)\n",
    "        all_dones.append(dones)\n",
    "        print(\"Episode\", i, \", cum rew: \", cum_rew)\n",
    "    print((\"Average rew:\", np.mean(perfs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_observations2, all_actions2, all_rewards2, all_dones2 = [] , [] , [] , []\n",
    "for i in range(len(all_dones)):\n",
    "    blank_dones = 200*[False]\n",
    "    blank_rewards = 200*[0.0]\n",
    "    blank_actions = np.zeros((200,39)).tolist()\n",
    "    blank_obs = np.zeros((200,86)).tolist()\n",
    "\n",
    "    blank_dones[:(len(all_dones[i])-1)] = all_dones[i][:-1]\n",
    "    blank_rewards[:(len(all_rewards[i])-1)] = all_rewards[i][:-1]\n",
    "    blank_actions[:(len(all_actions[i])-1)] = all_actions[i][:-1]\n",
    "    blank_obs[:(len(all_observations[i])-1)] = all_observations[i][:-1]\n",
    "\n",
    "    all_observations2.append(blank_obs)\n",
    "    all_actions2.append(blank_actions)\n",
    "    all_rewards2.append(blank_rewards)\n",
    "    all_dones2.append(blank_dones)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 120\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1920\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      " 26%|██▌       | 500/1920 [12:23<35:29,  1.50s/it]Saving model checkpoint to output/checkpoint-500\n",
      "Configuration saved in output/checkpoint-500/config.json\n",
      "Model weights saved in output/checkpoint-500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1983, 'learning_rate': 8.217592592592593e-05, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1000/1920 [24:46<23:45,  1.55s/it]Saving model checkpoint to output/checkpoint-1000\n",
      "Configuration saved in output/checkpoint-1000/config.json\n",
      "Model weights saved in output/checkpoint-1000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0371, 'learning_rate': 5.3240740740740744e-05, 'epoch': 62.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1500/1920 [37:10<10:06,  1.44s/it]Saving model checkpoint to output/checkpoint-1500\n",
      "Configuration saved in output/checkpoint-1500/config.json\n",
      "Model weights saved in output/checkpoint-1500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0314, 'learning_rate': 2.4305555555555558e-05, 'epoch': 93.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [47:36<00:00,  1.35s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1920/1920 [47:36<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2856.5965, 'train_samples_per_second': 42.008, 'train_steps_per_second': 0.672, 'train_loss': 0.07601597805817922, 'epoch': 120.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1920, training_loss=0.07601597805817922, metrics={'train_runtime': 2856.5965, 'train_samples_per_second': 42.008, 'train_steps_per_second': 0.672, 'train_loss': 0.07601597805817922, 'epoch': 120.0})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "##### CREATE DATASET OBJECT WITH TRAJECTORIES ####\n",
    "\n",
    "experience_dict = {'observations': all_observations2, 'actions': all_actions2, 'rewards': all_rewards2, 'dones': all_dones2}\n",
    "df = pd.DataFrame.from_dict(experience_dict)\n",
    "da = datasets.Dataset(pa.Table.from_pandas(df))\n",
    "dataset = datasets.DatasetDict({'train': da})\n",
    "\n",
    "\n",
    "#### create trainable\n",
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "\n",
    "\n",
    "#### train\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"output/\",\n",
    "remove_unused_columns=False,\n",
    "num_train_epochs=120,\n",
    "per_device_train_batch_size=64,\n",
    "learning_rate=1e-4,\n",
    "weight_decay=1e-4,\n",
    "warmup_ratio=0.1,\n",
    "optim=\"adamw_torch\",\n",
    "max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2371e+00  4.3647e-01 -3.3807e-01 -1.0207e-01 -6.0633e-01  6.5817e-01\n",
      "  2.4875e-01  8.5003e-01  2.3608e-01 -3.1889e-02 -7.1316e-03 -1.7548e-03\n",
      "  2.6401e-01 -2.0834e-02 -1.1196e-02  5.7523e-01  2.2938e-01 -2.8528e-03\n",
      " -3.2417e-03  1.2535e-01 -2.1219e-01  3.2079e-02 -7.2300e-03 -2.6391e-01\n",
      " -5.0825e-01  1.4052e+00 -9.6182e-05  2.0225e-05 -8.5985e-05 -2.6490e-01\n",
      " -5.0962e-01  1.4025e+00 -1.4433e-04  1.0323e-04 -1.8574e-05 -2.6355e-01\n",
      " -5.1052e-01  1.4016e+00 -2.6319e-01 -5.0821e-01  1.4016e+00  3.6019e-04\n",
      " -2.2744e-03 -3.6508e-03  1.7167e-03  1.4139e-03 -8.8827e-04  7.4164e-01\n",
      "  3.8710e-02  1.4413e-03  1.4277e-03  9.2832e-04  6.0979e-01  9.8122e-03\n",
      "  5.8737e-03  5.8451e-02  6.1799e-02  1.4269e-03  1.5639e-02  3.4160e-03\n",
      "  6.6882e-04  1.7018e-03  2.3003e-02  3.4791e-03  6.6964e-02  4.8909e-01\n",
      "  7.6082e-01  3.9991e-01  1.0915e-02  1.4705e-01  1.5584e-01  1.4539e-02\n",
      "  4.9274e-01  2.3028e-01  2.4338e-01  8.9980e-01  3.2195e-01  4.2762e-02\n",
      "  2.4291e-01  6.7887e-02  3.2464e-02  6.8648e-01  1.7865e-01  7.9837e-01\n",
      "  1.6311e-01  2.1682e-01]\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "model = model.to(\"cpu\")\n",
    "env = eval_env\n",
    "max_ep_len = 200\n",
    "device = \"cpu\"\n",
    "scale = 1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 1000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 15.0)\n",
      "(27, 15.0)\n",
      "(30, 20.0)\n",
      "(35, 15.0)\n",
      "(30, 10.0)\n",
      "(30, 15.0)\n",
      "(29, 20.0)\n",
      "(29, 15.0)\n",
      "(30, 15.0)\n",
      "(30, 15.0)\n"
     ]
    }
   ],
   "source": [
    "# Interact with the environment and create a video\n",
    "\n",
    "for _ in range(10):\n",
    "    done = False\n",
    "    episode_return, episode_length = 0, 0\n",
    "    state = env.reset()\n",
    "    target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "\n",
    "    while not done:\n",
    "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "        action = get_action(\n",
    "            model,\n",
    "            (states - state_mean) / state_std,\n",
    "            actions,\n",
    "            rewards,\n",
    "            target_return,\n",
    "            timesteps,\n",
    "        )\n",
    "        actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "        states = torch.cat([states, cur_state], dim=0)\n",
    "        rewards[-1] = reward\n",
    "\n",
    "        pred_return = target_return[0, -1] - (reward / scale)\n",
    "        target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "        timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            print((episode_length,episode_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTransformerGymDataCollator(return_tensors='pt', max_len=20, state_dim=86, act_dim=39, max_ep_len=200, scale=1000.0, state_mean=array([-1.2633e+00,  4.4615e-01, -3.4541e-01, -1.0592e-01, -6.1978e-01,\n",
       "        6.7275e-01,  2.5072e-01,  8.7233e-01,  2.4052e-01, -3.2339e-02,\n",
       "       -6.9329e-03, -1.7060e-03,  2.6980e-01, -2.1272e-02, -1.1475e-02,\n",
       "        5.8714e-01,  2.3457e-01, -2.8897e-03, -3.3776e-03,  1.2863e-01,\n",
       "       -2.1567e-01,  3.2636e-02, -7.3457e-03, -2.6959e-01, -5.1934e-01,\n",
       "        1.4361e+00, -9.4730e-05,  2.0428e-05, -9.0390e-05, -2.7079e-01,\n",
       "       -5.2087e-01,  1.4333e+00, -1.4577e-04,  1.0605e-04, -3.0901e-05,\n",
       "       -2.6922e-01, -5.2170e-01,  1.4324e+00, -2.6902e-01, -5.1940e-01,\n",
       "        1.4324e+00,  3.6528e-04, -2.3533e-03, -3.7123e-03,  1.7673e-03,\n",
       "        1.4718e-03, -8.5813e-04,  7.5854e-01,  3.9588e-02,  1.5634e-03,\n",
       "        1.4589e-03,  9.6145e-04,  6.2333e-01,  1.0152e-02,  6.1522e-03,\n",
       "        5.9578e-02,  6.5014e-02,  1.5636e-03,  1.6392e-02,  3.5839e-03,\n",
       "        7.6061e-04,  1.9147e-03,  2.4152e-02,  3.5996e-03,  6.9583e-02,\n",
       "        5.0162e-01,  7.7810e-01,  4.0743e-01,  1.1125e-02,  1.5002e-01,\n",
       "        1.5900e-01,  1.5052e-02,  5.0562e-01,  2.3708e-01,  2.5202e-01,\n",
       "        9.1935e-01,  3.2851e-01,  4.3979e-02,  2.4763e-01,  6.9437e-02,\n",
       "        3.3103e-02,  7.0240e-01,  1.8212e-01,  8.1632e-01,  1.6961e-01,\n",
       "        2.2164e-01]), state_std=array([0.1123, 0.0534, 0.1138, 0.3941, 0.2156, 0.1387, 0.3332, 0.4297,\n",
       "       0.0685, 0.0258, 0.0332, 0.0583, 0.0259, 0.0172, 0.0073, 0.2024,\n",
       "       0.057 , 0.0094, 0.0072, 0.1495, 0.1249, 0.1317, 0.006 , 0.0198,\n",
       "       0.0145, 0.0069, 0.0012, 0.0008, 0.002 , 0.0155, 0.0181, 0.0102,\n",
       "       0.0015, 0.0012, 0.0023, 0.0204, 0.0182, 0.0075, 0.0167, 0.0195,\n",
       "       0.0096, 0.0054, 0.007 , 0.0057, 0.0064, 0.0054, 0.0084, 0.2507,\n",
       "       0.0994, 0.0162, 0.0088, 0.0068, 0.2439, 0.0679, 0.0322, 0.1528,\n",
       "       0.1775, 0.0159, 0.0716, 0.0198, 0.0114, 0.0205, 0.0751, 0.0159,\n",
       "       0.1297, 0.3293, 0.2612, 0.3389, 0.0318, 0.1964, 0.2058, 0.0419,\n",
       "       0.3667, 0.308 , 0.3539, 0.0427, 0.2684, 0.1052, 0.2727, 0.1294,\n",
       "       0.1256, 0.2937, 0.2159, 0.2082, 0.3332, 0.2454]), p_sample=array([0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.0006, 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0005,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006, 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.0008, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.0006, 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.0008, 0.0007, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.0009, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0008, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.0007, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.0007, 0.0008, 0.001 , 0.0006, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.0005,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0008, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0009, 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006, 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0008, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006, 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0006, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0007, 0.001 , 0.0006, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0009, 0.0007, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.0007, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.0006, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0006, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.0008, 0.001 , 0.001 , 0.0007, 0.001 , 0.001 ,\n",
       "       0.0007, 0.001 , 0.001 , 0.0006, 0.001 , 0.0007, 0.001 , 0.001 ,\n",
       "       0.001 , 0.001 , 0.001 , 0.001 , 0.0007, 0.0008, 0.001 , 0.001 ]), n_traj=1000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if np.sum(dataset_cheetah['train'][i]['dones'])>0:\n",
    "        print('fdsf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0000, -10.0000,  10.0000,   1.3274,  10.0000, -10.0000,  -1.8194,\n",
       "          -6.2619, -10.0000,  10.0000,   3.2320,  -0.0863, -10.0000,  10.0000,\n",
       "          10.0000, -10.0000, -10.0000,   5.6166,  10.0000,  -5.9207,  10.0000,\n",
       "          -2.1693,  10.0000,  10.0000,  10.0000,  10.0000,  10.0000, -10.0000,\n",
       "          10.0000,  10.0000, -10.0000, -10.0000,  10.0000, -10.0000,   3.9839,\n",
       "          10.0000, -10.0000,  10.0000,  10.0000,  10.0000,   5.1911,  10.0000,\n",
       "         -10.0000,   8.5148, -10.0000,  10.0000,  10.0000, -10.0000,  -3.9352,\n",
       "          -2.0958,  -4.3004,  -3.2520, -10.0000,  -2.2302,  -3.1617,  -3.0870,\n",
       "          -2.2083,  -1.8401,  -2.4877,  -4.2620,  -1.2069,  -2.8776,  -4.4549,\n",
       "          -3.8955,  -3.5442,  -6.6043, -10.0000,  -4.6399,  -5.4344,  -4.4733,\n",
       "          -5.0089,  -4.2003,  -5.5503,  -3.3655,  -2.7025, -10.0000,  -5.4791,\n",
       "          -2.3303,  -4.3623,  -3.1463,  -1.9821, -10.0000,  -4.2417, -10.0000,\n",
       "          -2.3016,  -4.6624]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(envs.normalize_obs(np.array((states - state_mean) / state_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b9c577d5b1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "envs.norm_reward(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10760.844891987741"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset_cheetah['train'][0]['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepRL2",
   "language": "python",
   "name": "deeprl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
